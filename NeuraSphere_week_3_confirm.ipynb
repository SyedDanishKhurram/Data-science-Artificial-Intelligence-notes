{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **DATA EXTRACTION,DATA EXTRACTION ++**\n",
        "\n",
        "\n",
        "\n",
        "*   CVSs files\n",
        "*   JSON\n",
        "*   RSS feeds\n",
        "*   Data scraping\n",
        "*   Extraction Via Restful APIs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JmlZ9ZcpSiUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**csv files**\n"
      ],
      "metadata": {
        "id": "bHkm539eT7FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO Read a csv file\n",
        "To work with CSV file we need to import csv"
      ],
      "metadata": {
        "id": "FdLRv627UUZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "5K67u0qvUDTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the open() function to open the CSV file . You can specify the file path as an argument"
      ],
      "metadata": {
        "id": "K5v4Z9PnVwcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"competitions.csv\") as csv_file:"
      ],
      "metadata": {
        "id": "aArRTwOAUIBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To read the data"
      ],
      "metadata": {
        "id": "pZQ044SJXETs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contents_of_file = csv.reader(csv_file)"
      ],
      "metadata": {
        "id": "w-Y2PEruV9c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here **For** is used for a loop"
      ],
      "metadata": {
        "id": "Rcp-hLqqX0MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    for row in csv_reader:\n",
        "      print(row)\n"
      ],
      "metadata": {
        "id": "9kD7bBWHXz0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the information you wish to use to the CSV file. Make it into a list of lists or a list of dictionaries, with each inner list or dictionary standing in for a row of data."
      ],
      "metadata": {
        "id": "jQPi7mRkZkXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_write = [['Alice', 25], ['Bob', 30], ['Charlie', 22]]"
      ],
      "metadata": {
        "id": "yh0mNIF1ZHRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the CSV File for Writing: Use the open() function to open the CSV file in write mode ('w'). You can specify the file path as an argument."
      ],
      "metadata": {
        "id": "F3CJ1zKHawAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output.csv', 'w', newline='') as csv_file:\n",
        "# Note the newline='' argument; it helps ensure proper line endings in the CSV file."
      ],
      "metadata": {
        "id": "FWBN4UqeZHe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a csv.writer object by passing the opened file to it. This writer allows you to write rows of data to the CSV file."
      ],
      "metadata": {
        "id": "L8dIPjz8cCXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "#example\n",
        "import csv\n",
        "\n",
        "# Open a CSV file for writing\n",
        "with open('output.csv', 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write rows of data\n",
        "    csv_writer.writerow(['Name', 'Age'])\n",
        "    csv_writer.writerow(['Alice', 25])\n",
        "    csv_writer.writerow(['Bob', 30])\n",
        "    csv_writer.writerow(['Charlie', 22])\n"
      ],
      "metadata": {
        "id": "R2lqNz0aczM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the writer object to write each row of data to the CSV file using the writerow() method. In this example, we write each inner list as a row."
      ],
      "metadata": {
        "id": "iOe08cJ9dX7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for row in data_to_write:\n",
        "        csv_writer.writerow(row)"
      ],
      "metadata": {
        "id": "eUf7nMsfdXwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE TO READ DATA"
      ],
      "metadata": {
        "id": "ujMTJBj9dXlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file in read mode\n",
        "with open('data.csv', 'r') as csv_file:\n",
        "    # Create a CSV reader\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Iterate through rows and process data\n",
        "    for row in csv_reader:\n",
        "        # Access and process data in each row\n",
        "        print(row)"
      ],
      "metadata": {
        "id": "Mc-JxC53dXHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE TO WRITE DATA"
      ],
      "metadata": {
        "id": "uO7jKDQudW05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Prepare the data to write\n",
        "data_to_write = [['Alice', 25], ['Bob', 30], ['Charlie', 22]]\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open('output.csv', 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write data rows\n",
        "    for row in data_to_write:\n",
        "        csv_writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "mzvjVfYKdWmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JSON**\n",
        "\n",
        "**JSON (JavaScript Object Notation)**: A lightweight, text-based data interchange format that's easy for humans to read and write, and easy for machines to parse and generate.\n",
        "\n",
        "*Code* *Structure*:\n",
        "\n",
        "Objects: Enclosed in curly braces { } and can contain multiple key-value pairs.\n",
        "Arrays: Ordered lists enclosed in square brackets [ ].\n",
        "Key-Value Pairs: Strings associated with values, separated by a colon."
      ],
      "metadata": {
        "id": "d-eDoLN-dWZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"name\": \"John\",\n",
        "    \"age\": 30,\n",
        "    \"isStudent\": false,\n",
        "    \"courses\": [\"math\", \"science\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "tnRgjTAb3JPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this structure, \"name\", \"age\", and \"isStudent\" are keys, and they each have associated values. The \"courses\" key has an array of strings as its value."
      ],
      "metadata": {
        "id": "PA7ndzHR6vHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSS Feed (Really Simple Syndication)**: a web feed that provides access to website updates in a standardised, machine-readable format for users and applications. News websites, blogs, and other online publishers frequently utilise it to syndicate their content.\n",
        "\n",
        "**Code** **Structure**: RSS is XML-based. Here's a simplified structure of what an RSS feed might look like:"
      ],
      "metadata": {
        "id": "QWK_4ISX6wjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
        "<rss version=\"2.0\">\n",
        "    <channel>\n",
        "        <title>Feed Title</title>\n",
        "        <link>http://www.example.com/</link>\n",
        "        <description>Description of the feed</description>\n",
        "        <lastBuildDate>Date of the last update</lastBuildDate>\n",
        "\n",
        "        <item>\n",
        "            <title>Entry Title</title>\n",
        "            <link>http://www.example.com/link-to-entry</link>\n",
        "            <description>Summary of the entry</description>\n",
        "            <pubDate>Publication Date</pubDate>\n",
        "        </item>\n",
        "        <!-- Additional items can be added similarly -->\n",
        "    </channel>\n",
        "</rss>\n"
      ],
      "metadata": {
        "id": "IF5qmOIE6M7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tb2TpmiZ_d3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "here are the following keys which is used in above\n",
        "<rss>: This is the root element. It contains a version attribute specifying the version of RSS being used.\n",
        "\n",
        "\n",
        "<channel>: This is a required child element of <rss>, containing metadata about the feed itself and housing the individual content items.\n",
        "\n",
        "\n",
        "<title>: The title of the channel (e.g., the name of your website) or the title of an individual item.\n",
        "\n",
        "\n",
        "<link> =   The URL to the HTML website corresponding to the channel or the URL of an individual item.\n",
        "\n",
        "\n",
        "<description> : A brief summary of the channel or a brief summary or the content of an individual item.\n",
        "\n",
        "\n",
        "<item>: This represents individual stories or posts within the RSS feed. A <channel> can have multiple <item> elements.\n",
        "\n",
        "<pubDate>: This is the publication date for content in <item>. It's useful for readers to know when the content was published.\n",
        "\n",
        "<lastBuildDate>: Date and time the content of the channel was last changed."
      ],
      "metadata": {
        "id": "6bZPHmv8_yOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Scraping** = The practise of removing data for retrieval, analysis, or storage from websites or other sources is known as data scraping, sometimes known as web scraping. It allows for the collection of precise data without the need for manual copy-pasting or retyping of data. Here is a quick summary:\n",
        "\n",
        "**Python Libraries:**\n",
        "\n",
        "BeautifulSoup: Used for parsing HTML and XML documents.\n",
        "\n",
        "Scrapy: An open-source framework for extracting data from websites.\n",
        "\n",
        "Requests: For making HTTP requests in Python.\n",
        "\n",
        "**Browser Extensions:** Tools like \"Web Scraper\" can be added to browsers to easily scrape web data without coding.\n",
        "\n",
        "**Software**: Tools like \"Octoparse\" or \"Import.io\" provide GUI-based platforms for data scraping.\n",
        "\n",
        "**Ethical and  ConcLegalerns**:\n",
        "**Robots.txt**: Many websites have a robots.txt file that specifies which parts of the site can be scraped or accessed by web robots. Respecting this file is an important ethical practice.\n",
        "\n",
        "**Rate Limits**: Bombarding websites with rapid and repeated requests can crash websites. Respect any rate limits a website may have in place.\n",
        "\n",
        "**Privacy Concerns**: Scraping personal data can raise privacy concerns and may be illegal in many jurisdictions.\n",
        "\n",
        "**Copyright and Terms of Service**: Just because data is publicly available on a website doesn't mean it's legal to scrape, use, or republish it. Always review a website's terms of service before scraping.\n",
        "\n",
        "4. **Potential Challenges**:\n",
        "Dynamic Content: Websites using JavaScript to load content dynamically can pose challenges for traditional scraping methods. Solutions might involve using browser automation tools like Selenium to render dynamic pages.\n",
        "\n",
        "**CAPTCHAs**: Some websites implement CAPTCHAs to prevent automated access, including scraping.\n",
        "\n",
        "**Structural Changes**: Websites might change their structure, which can break scraping scripts. Regular maintenance and updates of scraping scripts might be needed.\n",
        "\n",
        "Data scraping can be a powerful method to retrieve data for analysis, especially when APIs or datasets aren't available. However, it's essential to approach it responsibly, ethically, and legally.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8VRFw8iNA7Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL to scrape from\n",
        "url = \"http://quotes.toscrape.com\"\n",
        "\n",
        "# Sending a request to the website and getting the content\n",
        "response = requests.get(url)\n",
        "page_content = response.content\n",
        "\n",
        "# Initializing BeautifulSoup and specifying the parser\n",
        "soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "# Extracting quotes using their class name\n",
        "quotes = soup.find_all('span', class_='text')\n",
        "\n",
        "# Printing the quotes\n",
        "for quote in quotes:\n",
        "    print(quote.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIpRrk77Cm_M",
        "outputId": "1b1599e3-15ff-4e8a-9114-7cb4734a5cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
            "“Try not to become a man of success. Rather become a man of value.”\n",
            "“It is better to be hated for what you are than to be loved for what you are not.”\n",
            "“I have not failed. I've just found 10,000 ways that won't work.”\n",
            "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
            "“A day without sunshine is like, you know, night.”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "Remember, before scraping any website:\n",
        "\n",
        "Check the robots.txt of the website to ensure you're allowed to scrape it.\n",
        "Review the website's terms of service.\n",
        "Ensure you're not violating any laws, especially related to data privacy.\n",
        "Also, note that if a website changes its structure (e.g., its HTML tags and attributes), the scraper might stop working and will need to be adjusted to the new structure.\n",
        "\n",
        "Lastly, always be respectful: avoid hammering websites with too many rapid requests, which could slow down or crash the server.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ErDx_myJ9tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction via Restful API's\n",
        "Extracting data via RESTful APIs is a more formalized and efficient way to obtain data compared to web scraping. APIs (Application Programming Interfaces) are designed to allow for programmatic access to a system's data and functions, and REST (Representational State Transfer) is an architectural style commonly used in web services.\n",
        "\n",
        "Here are the basics of data extraction via RESTful APIs:\n",
        "\n",
        "1. Basics of RESTful API:\n",
        "Endpoints: URLs that specify where API requests are directed.\n",
        "HTTP Methods: Determine the type of the operation. Common methods include:\n",
        "GET: Retrieve data.\n",
        "POST: Submit data.\n",
        "PUT: Update or replace data.\n",
        "DELETE: Remove data.\n",
        "Status Codes: Indicate the success or failure of a request (e.g., 200 OK, 404 Not Found).\n",
        "2. Making a Request:\n",
        "Most programming languages have libraries or tools to make HTTP requests. In Python, the popular requests library can be used."
      ],
      "metadata": {
        "id": "Qiw55NywL7o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#python example\n",
        "\n",
        "import requests\n",
        "\n",
        "url = 'https://api.github.com/users/octocat'  # Endpoint for user 'octocat'\n",
        "\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "print(data['name'])  # Print the name of the user\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKU_qB2sKTqK",
        "outputId": "6db66be0-e86e-4fdb-ab4f-d20df2b7098c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Octocat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Authentication:\n",
        "Many APIs require authentication to limit usage and protect data. This can be done in various ways:\n",
        "\n",
        "API Keys: A token passed with each request (either in headers or as a URL parameter).\n",
        "OAuth: A more complex authentication process often used for services that require various permissions.\n",
        "5. Rate Limits:\n",
        "APIs often have rate limits to prevent abuse. For instance, you may be limited to 1,000 requests per day or 30 requests per minute. Always check an API's documentation to know these limits.\n",
        "\n",
        "6. Handling Responses:\n",
        "API responses can be in various formats, but JSON (JavaScript Object Notation) is the most common. Libraries like Python's requests make it easy to parse JSON responses into dictionaries.\n",
        "\n",
        "7. Best Practices:\n",
        "Always refer to the API documentation to understand its capabilities and limitations.\n",
        "Handle potential errors gracefully, e.g., network errors, rate limits exceeded, data not found.\n",
        "Cache responses when appropriate to reduce the number of necessary API calls.\n",
        "8. Benefits over Web Scraping:\n",
        "Stability: APIs typically offer a stable interface, while web scraping can break if a website changes its structure.\n",
        "Efficiency: Direct access to structured data without the need for parsing HTML.\n",
        "Ethics and Legality: APIs are provided explicitly for data access, reducing ethical and legal concerns."
      ],
      "metadata": {
        "id": "2Ru632QuMhDv"
      }
    }
  ]
}